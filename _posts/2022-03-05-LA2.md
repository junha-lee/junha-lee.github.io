---
title: 선형대수
date: 2022-03-05
category: study
tags:
    - DA
    - Linear Algebra
toc: true
author_profile: false
sidebar:
  nav: "docs"


---

## 선형대수란

- 선형대수는 vector와 matrix로 숫자를 표현하고 연산하는 수학의 한 분야
- 주요 구성요소
  - scalra : 하나의 숫자
  - vector : 여러개의 숫자를 한줄로 배열 한 것
  - matrix : 여러개의 숫자를 사각형 형태로 배열한 것
  - Tensor : 컴퓨터 프로그래밍에서는 n-D array를 tensor라고 표현

## 벡터

* 원소 (element): 벡터를 구성하고 있는 각 숫자

* 벡터의 차원: 벡터에 포함된 원소의 수

* 방향성: 원점으로부터의 방향

* 하나의 vector는 N 차원 공간 상의 점 (point)을 의미

* 위치는(즉, vector의 위치)는 vector의 원소값에 의해 결정

* 원소의 값은 해당 벡터의 고유한 특성
  * 벡터가 공간상에서 갖는 위치는 해당 벡터의 고유한 특성을 반영 
  
* 위치가 비슷한 정도 → 거리로 계산 (가까울수록 더 유사)

* Euclidean method : np.linalg.norm()

* 벡터 간의 합과 차 : 같은 자리에 있는 원소끼리 더하거나 혹은 뺀다

* 데이터 분석에서의 벡터 : 각 관측치의 변수 정보를 이용하여 관측치를 하나의 벡터로 표현

* 방향이 유사할수록 유사도가 높다

* 두 벡터의 방향이 유사한 정도는 사이각을 이용해서 표현 ( 이를 표현하기 위해 cos 함수 사용 )

* 내적 : 같은 자리에 있는 원소들을 곱하여 더한다.

* a▪b=|a||b|cosθ -> cosθ = a▪b/|a||b| -> np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))

* cosine distance = 1 – cosθ

* 코사인 유사도는 벡터의 규모(크기)가 중요하지 않을때 사용되며, 상호 상관관계를 가지는 원소들간의 유사도를 계산할때 좋지 않다.

* L1 norm : 벡터의 모든 성분의 절대값을 더함

  ![img](https://blog.kakaocdn.net/dn/bxZUkv/btqE7A90oUh/moe6CK7ZemwhuXIVUK8jTK/img.png)

* L2 norm은 출발점에서 도착점까지의 거리를 직선거리로 측정

  ![img](https://blog.kakaocdn.net/dn/Nh0mX/btqE8xShb9Z/UCmpkRHZxTeqKH1tSgsX90/img.png)

## 행렬

![image-20220305220225519](../../assets/images/2022-03-05-LA2/image-20220305220225519.png)

* 행렬과 벡터 곱의 기하학적 의미
* 행렬 A는 공간상의 한 점(벡터)을 다른 점으로 이동시키는 역할

### 대각 행렬 (diagonal matrix)

* 대각선 위에 있는 원소 이외의 다른 원소의 값이 모두 0인 행렬

### 단위행렬 (unit matrix 또는 identity matrix)

* 대각 성분이 모두 1인 대각행렬
* 단위행렬에 의해 행해지는 변환은 벡터를 움직이지 않는 변환

### 전치행렬 (transposed matrix)

* 특정 행렬의 전치행렬은 행과 열이 바뀐 행렬

![image-20220306002413006](../../assets/images/2022-03-05-LA2/image-20220306002413006.png)

### 대칭행렬 (symmetric matrix)

* 대각선을 기준으로 위와 아래가 같은 행렬

* $$
  A = A^T
  $$

* 

## 데이터 분석에서의 행렬

* 기본역할 : 여러개의 데이터 포인트에 대한 값을 저장

* 연립방정식 풀이

  *  ![image-20220306232204446](../../assets/images/2022-03-05-LA2/image-20220306232204446.png)
    $$
    x = A^{-1}*y
    $$

    * x = np.dot(np.linalg.inv(A), y)

* optimization

* 데이터 차원 축소

## 역행렬의 기하학적 의미

* A의 역행렬은 A에 의해서 옮겨진 점을 원래의 점으로 옮기는 변환을 의미
* 역행렬이 존재하면 유일한 해 존재

## Rank

* 행렬 A의 rank는 서로 선형 독립인 (linearly independent) A의 행의 갯수 (또는 열의 갯수)를 의미

* $$
  A= \left[
  \begin{matrix}
      1 & 2 \\
      2 & 5 \\
  \end{matrix}
  \right]
  
  , rank(A) = 2
  $$

  

  * (2, 5)는 (1,2)의 실수배로 나타낼 수 없으므로 두 행은 서로 독립, rank(A) = 2

* Full rank matrix : nxn A 행렬에 대해서 모든 행이 서로 선형 독립이거나 혹은 모든 열이 서로 선형 독립인 경우

* full rank가 아닌 경우 (singular인 경우)의 예 : 
  $$
  𝑥_1+2𝑥_2=1, 2𝑥_1+4𝑥_2=2
  $$
  

$$
A= \left[
\begin{matrix}
    1 & 2 \\
    2 & 4 \\
\end{matrix}
\right]

, rank(B) = 1
$$

* Ax=y 에서 A 의 역행렬이 없다는 것의 의미
  $$
  \left[
  \begin{matrix}
      a & b \\
      c & d \\
  \end{matrix}
  \right]
  \left[
  \begin{matrix}
      x_1 \\
      x_2 \\
  \end{matrix}
  \right] = 
  \left[
  \begin{matrix}
      y_1 \\
      y_2 \\
  \end{matrix}
  \right]
  에 대해서 ab-bc = 0인 경우
  $$

  $$
  {𝑎\over 𝑐} =  {𝑦_1 \over 𝑦_2 } 이면부정, 아니면불능
  $$

  * 부정의 기하학적 의미 : y로 이동한 x가 여러개 있기 때문에 y를 가지고 원래의 유일한 x를 찾을 수 없다.
  * 불능의 기하학적 의미 : A변환에 의해 이동될 수 없는 점

* 직사각형의 역행렬

## 고유값과 고유벡터

*  정의

  * Av= λv (A는 **nxn** **행렬**, v는 nx1 벡터 (≠영벡터), λ는 스칼라값)
  * 위의 식을 만족하는v를 A의 고유벡터 (v≠0) , λ를 A의 고유값
*  기하학적 의미

  * 고유벡터 : 행렬 A에 의해 선형변환되는 경우 방향은 바뀌지 않고, 길이만 달라지는 벡터
  * 즉, Av는v벡터의 방향은 바꾸지 않고, 크기만 변형하는 것
  * 고유벡터는 방향성만이 중요하기 때문에 여러개가 나올 수 있지만, 보통 그 길이가 1인 고유벡터를 선택
  * eigVals, eigVecs = np.linalg.eig(A)

* 고유값의 특성
  * 행렬식 (determinant) = 고유값들의 곱 (full rank의 경우 고유값들의 곱 = 0)
  * 고유값들의 합 = 대각 성분의 합



