---
title: 선형대수(2)
date: 2022-03-08
category: study
tags:
    - DA
    - Linear Algebra
toc: true
author_profile: false
sidebar:
  nav: "docs"


---

## 행렬분해

- 하나의 행렬을 여러개의 서로 다른 행렬의 곱으로 표현하는 것
- Why use?
  - 계산을 쉽게 하기 위해서 - 특히, 어떠한 행렬을 여러번 반복해서 곱하는 경우, 행렬 분해를 사용하면 쉽게 계산 가능
  - 기계학습과 같은 데이터 분석에서 사용

## LU 분해

* 아래 삼각행렬 (lower triangle matrix)과 위 삼각행렬 (upper triangle matrix)로 분해

  ![image-20220308171728011](../../assets/images/2022-03-08-LA3/image-20220308171728011.png)

## QR 분해

- A = QR
- A: mxn rectangular matrix
- Q: mxm orthogonal 행렬, 각 행이 모두 선형독립이고 90도를 이룸
- R: mxn upper triangle matrix

## NMF

* A = BC
* A, B, C 모두 non-negative matrix 즉, 음수인 원소가 없는 행렬들

## 고유 분해

- A=VΛV^(-1)

- A: nxn 정사각행렬
- V: A의 고유벡터들을 열로 갖는 행렬
- Λ: 고유값들을 대각성분으로 갖는 대각행렬
- nA=VΛV^(-1)은 언제 사용할 수 있는가? 
  * A 변환이 여러번 수행되는 경우를 간단하게 계산 가능
    * A2 = AA = VΛV^(-1) VΛV^(-1) = VΛ^2 V^(-1) , = Λ^2
- PCA 차원 축소
  - 원데이터의 각 독립변수에 대해서 mean centering 한다 (즉, 원래 값에서 해당 변수의 평균을 뺀다).
  - 원데이터에 대한 공분산 행렬을 만든다.
  - 공분산 행렬에 대해서 고유분해를 수행한다 (즉, 고유값과 고유벡터를 찾는다).
  - 각 고유벡터가 우리가 찾고자 하는 PC가 된다. 
  - 우리는 이중에서 설명력이 높은 PC만을 선택한다. 
    - 몇개를 선택하는지는 사용자가 결정 
    - 이렇게 선택된 PC가 우리가 최종적으로 사용하고자 하는 독립변수가 됨 (즉, feature 가 됨)
  - 새로 구한 PC에 대해 각 관측치의 새로운 값 구하기

## 차원 축소

* 차원의 저주 : 높은 차원에서 과적합되어, 결과가 좋지 않다.

* 해결 

  * Feature selection : 원래의 features 들 중에서 일부만 선택

    * 단점: 선택되지 않은 features가 갖고 있는 정보를 최종 분석에서 사용하지 못한다. 

  * Feature extraction : 원래 feature들을 그대로 사용하는 것이 아니라, 원래 feature들이 가지고 있는 정보를 사용하여 새로운 feature들을 추출

    * 장점 : 데이터셋에 존재하는 feature들을 버리지 않고, feature들이 가지고 있는 많은 정보를 사용할 수 있다.

      

## Principal Component Analysis (주성분분석)

* Principal component : 원 데이터가 가지고 있는 정보, 즉, 원 독립변수들이 가지고 있는 정보를 설명하는 축들
* Principal component 들 중에서 분산을 많이 설명하는 상위 몇개의 PC만을 선택하여 사용 
* 이를 이용하여 원데이터를 다시 표현
* 효과: 원 데이터의 정보 (즉, 분산)은 별로 손실하지 않으면서 feature수를 줄이는 효과

![image-20220310154248417](../../assets/images/2022-03-08-LA3/image-20220310154248417.png)





