---
title: DDPG
date: 2021.02.17
category: project
tags:
    - RL
    - actor-cretic
    - DDPG
toc: true
author_profile: false
sidebar:
  nav: "docs"
---

## actor-cretic

- REINFORCE 알고리즘의 단점 = 에피소드 마다 학습이 이루어진다. (variance, non on-line)

->정책 이터레이션 구조 사용 ( 정책 발전과 정책 평가를 각각 수행)

- REINFORCE 알고리즘에서 Gt를 다시 큐함수로 대체 ( 정책 평가를 위해 ) 하고, 큐함수를 알 수 없으므로 인공신경망을 사용하여 근사

  ![image-20220222014704959](../../assets/images/2022-02-21-handwrite%20(copy)/image-20220222014704959.png)

- REINFORCE 알고리즘은 반환 값을 구하기 위해 매 에피소드 마다 업데이트 했지만, Actor-Cretic은 큐함수로 대체하여 매 스텝마다 업데이트 가능.
- ∴ 비교적 샘플들의 분산이 적지만, 에피소드 끝까지 가지 않고, 평가를 근사하는 방식을 사용하기에 편향이 생긴다. 



## a2c

- DQN에서는 MSE 오류 함수를 사용한 반면 Actor-cretic은 정답을 알 수 없어,큐 함수가 그대로 크로스 엔트로피에 곱해지고, 때문에 큐 함수의 값에 따라 오류 함수의 값이 크게 변하여 분산이 크다. 이에 큐함수에 베이스 라인을 두어 큐함수의 값이 크게 다르지 않게 해야 하며, 베이스라인으로 가치함수를 근사하여 사용하며, 이를 Advantage 함수라고 함.

![image-20220222015045145](../../assets/images/2022-02-21-handwrite%20(copy)/image-20220222015045145.png)

![image-20220222014848782](../../assets/images/2022-02-21-handwrite%20(copy)/image-20220222014848782.png)

( 큐함수를 가치함수를 통해 표현 à 큐함수 근사 X , 시간차 에러 )

- 정책신경망을 통해 행동 선택 -> 선택한 행동으로 한 스텝 진행 -> 

  환경으로 부터 다음상태, 보상 받음 ->

  샘플( s, a, r, s’)을 통해  Advantage 함수 구함- >

   MSE로 가치신경망 업데이트, Advantage 함수로 정책신경망 업데이트

## a3c

- 앞서 DQN에서는 안 좋은 상황에 빠진 딥 살사가 그 상황에 맞게 학습한다는 단점을 보완하고자 리플레이 메모리를 사용하여 샘플 간의 시간적 상관관계 제거

- A2C에서는 샘플 사이의 시간적 상관관계 제거를 위한 방법으로 비동기 업데이트 제시

  ![image-20220222015142231](../../assets/images/2022-02-21-handwrite%20(copy)/image-20220222015142231.png)

- 에이전트를 여러 개 사용하여 각각 다른 trajectory를 거치며, 설정한 step 동안 수행

- 각각의 에이전트는 일정 step 끝나면, global network 업데이트

- global network로 자신을 업데이트


## DDPG

- 대부분 문제는 연속적이고, 고차원이지만, DQN은 action을 찾는 방법으로 action-value function을 최대화 하여,  Low-dimentional action spaces 문제만 다룰 수 있다.
- ∴Actor-Critic에 기초한 DPG 알고리즘 (DDPG)를 사용한다.
- 특징 : Model free, off-policy, continuous actor-critic, replay buffer
- DQN과의 차이 : 두 개의 네트워크, soft target update
- output action에 noise를 추가하여 Exploration문제를 학습 알고리즘으로 부터 독립 적으로 다룰 수 있다.

## 논문 요약

![image-20220222015318843](../../assets/images/2022-02-21-handwrite%20(copy)/image-20220222015318843.png)

- **평가 지표**
  - on/off policy
  - DDPG-based control policy
  - P1의 모든 제약 조건이 충족될 때 제안된 알고리즘의 성능에 대한 하한 

## 구현

- Data :
  - nonshiftable power demand,
  - outdoor temperature,
  - and electricity price

![image-20220222015529361](../../assets/images/2022-02-21-handwrite%20(copy)/image-20220222015529361.png)

[https://en.wikipedia.org/wiki/Austin,_Texas#Climate](https://en.wikipedia.org/wiki/Austin,_Texas)



